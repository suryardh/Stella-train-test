{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0dOznT8/WlvO2xJtl5Qnf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryardh/Stella-train-test/blob/main/stella_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && apt upgrade -y\n",
        "!pip uninstall pyarrow -y\n",
        "!pip install pyarrow==15.0.0\n",
        "!apt install python3.8 python3.8-dev python3.8-distutils libpython3.8-dev\n",
        "!pip install tensorflow transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJn5BJHQh8No",
        "outputId": "1fed0a97-8b82-4eea-955b-c5ad1083197a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "4 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Found existing installation: pyarrow 15.0.0\n",
            "Uninstalling pyarrow-15.0.0:\n",
            "  Successfully uninstalled pyarrow-15.0.0\n",
            "Collecting pyarrow==15.0.0\n",
            "  Using cached pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==15.0.0) (1.26.4)\n",
            "Using cached pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "Installing collected packages: pyarrow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-15.0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libpython3.8-dev is already the newest version (3.8.19-1+jammy1).\n",
            "python3.8 is already the newest version (3.8.19-1+jammy1).\n",
            "python3.8-dev is already the newest version (3.8.19-1+jammy1).\n",
            "python3.8-distutils is already the newest version (3.8.19-1+jammy1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "U1PzEroTgOlM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Lots-of-LoRAs/task1533_daily_dialog_formal_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Lots-of-LoRAs/task1533_daily_dialog_formal_classification\")\n",
        "print(ds.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-3hK5dPnyZU",
        "outputId": "a8b18136-b308-4478-8cda-83ed26eb85ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['input', 'output', 'id'], 'test': ['input', 'output', 'id'], 'valid': ['input', 'output', 'id']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.inverse_vocab = {}\n",
        "        self.current_id = 1  # Start from 1, leave 0 for padding or special tokens\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        # Count word frequencies\n",
        "        for text in texts:\n",
        "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "            for word in words:\n",
        "                word_freq[word] += 1\n",
        "\n",
        "        # Build vocab\n",
        "        for word in word_freq:\n",
        "            if word not in self.vocab:\n",
        "                self.vocab[word] = self.current_id\n",
        "                self.inverse_vocab[self.current_id] = word\n",
        "                self.current_id += 1\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Convert text to integer tokens based on vocab\n",
        "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "        return [self.vocab.get(word, 0) for word in words]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        # Convert integer tokens back to text\n",
        "        return ' '.join([self.inverse_vocab.get(token, '[UNK]') for token in tokens])\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab) + 1  # +1 for padding or special tokens\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"Lots-of-LoRAs/task1533_daily_dialog_formal_classification\")\n",
        "\n",
        "# Extract texts from the relevant column\n",
        "texts = ds['train']['input']  # Replace 'text' with the actual column name\n",
        "\n",
        "# Create and use the tokenizer\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab(texts)\n",
        "\n",
        "# Example of encoding and decoding\n",
        "sample_text = \"This is a sample sentence.\"\n",
        "encoded = tokenizer.encode(sample_text)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(\"Vocabulary Size:\", tokenizer.vocab_size())\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIqmezBtkZjz",
        "outputId": "699df836-c2e1-4e10-8830-a6dca16069de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 13024\n",
            "Encoded: [184, 25, 5, 4582, 6888]\n",
            "Decoded: this is a sample sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Assuming SimpleTokenizer is already defined and texts are extracted\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "        return torch.tensor(tokens), torch.tensor(label)\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"Lots-of-LoRAs/task1533_daily_dialog_formal_classification\")\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = ds['train']['input']  # Replace 'text' with the actual column name\n",
        "labels = ds['train']['output']  # Replace 'label' with the actual column name\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab(texts)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = TextDataset(texts, labels, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "4j86yWj3k8oi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCZ0gQuutrIL",
        "outputId": "b088c2a5-3e38-437d-bf3c-d66b336710f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "        return torch.tensor(tokens), torch.tensor(label)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = TextDataset(texts, encoded_labels, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "9gNS8lyYts96"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstrak data validasi\n",
        "valid_texts = ds['valid']['input']  # Ambil data input untuk validasi\n",
        "valid_labels = ds['valid']['output']  # Ambil data output untuk validasi\n",
        "\n",
        "# Encode label validasi\n",
        "encoded_valid_labels = label_encoder.transform(valid_labels)\n",
        "\n",
        "# Buat dataset dan dataloader untuk validasi\n",
        "valid_dataset = TextDataset(valid_texts, encoded_valid_labels, tokenizer)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_f9sXYUz1xM",
        "outputId": "0570e0d2-f334-4603-eb28-20262737c865"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Define EarlyStopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = np.inf\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Define the model, criterion, and optimizer\n",
        "model = SimpleModel(vocab_size, embed_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize early stopping\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for tokens, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tokens)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for tokens, labels in valid_dataloader:\n",
        "            outputs = model(tokens)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "\n",
        "    # Print training and validation losses\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}, Valid Loss: {avg_valid_loss}')\n",
        "\n",
        "    # Check early stopping\n",
        "    early_stopping(avg_valid_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Evaluate on test data\n",
        "model.eval()\n",
        "total_test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for tokens, labels in test_dataloader:\n",
        "        outputs = model(tokens)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "print(f'Test Loss: {avg_test_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZlxAnTZtwJT",
        "outputId": "9ea979dd-b7d2-4821-a8b6-0f9551e184b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 0.6648402083304621, Valid Loss: 0.6551888465881348\n",
            "Epoch 2/50, Train Loss: 0.6619510308388741, Valid Loss: 0.6536314308643341\n",
            "Epoch 3/50, Train Loss: 0.663918383659855, Valid Loss: 0.6539283126592637\n",
            "Epoch 4/50, Train Loss: 0.6594446595638029, Valid Loss: 0.6542297095060349\n",
            "Epoch 5/50, Train Loss: 0.6605251950602378, Valid Loss: 0.6497737884521484\n",
            "Epoch 6/50, Train Loss: 0.6555539185001004, Valid Loss: 0.6492236584424973\n",
            "Epoch 7/50, Train Loss: 0.6549634302816083, Valid Loss: 0.6466265052556992\n",
            "Epoch 8/50, Train Loss: 0.6509084047809723, Valid Loss: 0.6436367928981781\n",
            "Epoch 9/50, Train Loss: 0.6452099311736322, Valid Loss: 0.638955295085907\n",
            "Epoch 10/50, Train Loss: 0.6355782570377473, Valid Loss: 0.6351173341274261\n",
            "Epoch 11/50, Train Loss: 0.6284917889102812, Valid Loss: 0.6305531054735184\n",
            "Epoch 12/50, Train Loss: 0.6173550938406298, Valid Loss: 0.6249002635478973\n",
            "Epoch 13/50, Train Loss: 0.6056306529429651, Valid Loss: 0.6221257418394088\n",
            "Epoch 14/50, Train Loss: 0.5984429213308519, Valid Loss: 0.6158282518386841\n",
            "Epoch 15/50, Train Loss: 0.5804524339014484, Valid Loss: 0.6100331947207451\n",
            "Epoch 16/50, Train Loss: 0.5740761820347079, Valid Loss: 0.6083718508481979\n",
            "Epoch 17/50, Train Loss: 0.5602866245854285, Valid Loss: 0.6008981108665467\n",
            "Epoch 18/50, Train Loss: 0.543590009789313, Valid Loss: 0.5996154323220253\n",
            "Epoch 19/50, Train Loss: 0.5343781038638085, Valid Loss: 0.5929845422506332\n",
            "Epoch 20/50, Train Loss: 0.5220998041091427, Valid Loss: 0.594636869430542\n",
            "Epoch 21/50, Train Loss: 0.5090000414079235, Valid Loss: 0.5926251679658889\n",
            "Epoch 22/50, Train Loss: 0.4984672261822608, Valid Loss: 0.5880700945854187\n",
            "Epoch 23/50, Train Loss: 0.4851829803759052, Valid Loss: 0.5870031282305718\n",
            "Epoch 24/50, Train Loss: 0.4766435707769086, Valid Loss: 0.5879864826798439\n",
            "Epoch 25/50, Train Loss: 0.4654862513465266, Valid Loss: 0.585634657740593\n",
            "Epoch 26/50, Train Loss: 0.455185406438766, Valid Loss: 0.5860771328210831\n",
            "Epoch 27/50, Train Loss: 0.45404551798297516, Valid Loss: 0.5866932287812233\n",
            "Epoch 28/50, Train Loss: 0.43722669309185397, Valid Loss: 0.5948346823453903\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-7eedecb2c3b2>:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6277572944760322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model terbaik\n",
        "torch.save(model.state_dict(), 'Stella Model.pth')\n"
      ],
      "metadata": {
        "id": "fFcil2Ow0ztr"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}